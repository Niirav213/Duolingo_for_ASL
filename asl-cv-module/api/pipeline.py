"""
api/pipeline.py
---------------
Orchestrates the full CV pipeline in one place.
Loads all models ONCE at startup and wires:
    detector â†’ extractor â†’ classifier â†’ scorer â†’ feedback

This is the single entry point called by router.py
"""

import cv2
import base64
import numpy as np
import torch
from pathlib import Path

from core.detector import ASLDetector
from core.extractor import ASLFeatureExtractor
from core.scorer import ASLScorer
from feedback.generator import FeedbackGenerator, FeedbackResult
from models.static import StaticSignClassifier
from models.dynamic import DynamicSignClassifier
from api.schemas import AnalyzeFrameResponse, JointScores

# â”€â”€ Model registry â€” edit to swap models â”€â”€
MODEL_REGISTRY = {
    "static": "models/checkpoints/static_sign.pt",
    "dynamic": "models/checkpoints/dynamic_sign.pt",
}


class ASLPipeline:
    """
    Full ASL CV pipeline â€” instantiated ONCE at server startup.

    Call analyze_frame() for every incoming frame.
    """

    def __init__(
        self,
        load_static: bool = True,
        load_dynamic: bool = True,
        static_labels: list[str] = None,
        dynamic_labels: list[str] = None,
    ):
        print("[ASLPipeline] Initializing...")

        # â”€â”€ Core components â”€â”€
        self.detector = ASLDetector(model_complexity=1, draw_landmarks=False)
        self.extractor = ASLFeatureExtractor()
        self.scorer = ASLScorer()
        self.feedback_gen = FeedbackGenerator()

        # â”€â”€ Static classifier (A-Z letters) â”€â”€
        self.static_classifier = None
        if load_static:
            self.static_classifier = StaticSignClassifier(
                num_classes=26,
                labels=static_labels or list("ABCDEFGHIJKLMNOPQRSTUVWXYZ"),
            )
            static_path = MODEL_REGISTRY["static"]
            if Path(static_path).exists():
                self.static_classifier.load(static_path)
            else:
                print(f"[ASLPipeline] WARNING: No static checkpoint found at {static_path}. Using untrained model.")

        # â”€â”€ Dynamic classifier (word-level signs) â”€â”€
        self.dynamic_classifier = None
        if load_dynamic:
            self.dynamic_classifier = DynamicSignClassifier(
                num_classes=len(dynamic_labels) if dynamic_labels else 100,
                labels=dynamic_labels,
                seq_len=30,
            )
            dynamic_path = MODEL_REGISTRY["dynamic"]
            if Path(dynamic_path).exists():
                self.dynamic_classifier.load(dynamic_path)
            else:
                print(f"[ASLPipeline] WARNING: No dynamic checkpoint found at {dynamic_path}. Using untrained model.")

        print("[ASLPipeline] Ready.")

    def analyze_frame(
        self,
        frame_base64: str,
        target_sign: str,
        mode: str = "static",
        include_landmarks: bool = False,
    ) -> AnalyzeFrameResponse:
        """
        Full pipeline: base64 frame â†’ JSON response.

        Args:
            frame_base64: base64-encoded JPEG/PNG image string
            target_sign: the ASL sign the user is attempting
            mode: "static" for letters, "dynamic" for word signs
            include_landmarks: whether to include raw landmarks in response

        Returns:
            AnalyzeFrameResponse â€” ready to serialize to JSON
        """
        # â”€â”€ 1. Decode frame â”€â”€
        frame = self._decode_frame(frame_base64)

        # â”€â”€ 2. Detect landmarks â”€â”€
        detection = self.detector.process_frame(frame)

        if not detection.is_valid():
            return self._no_detection_response(target_sign)

        # â”€â”€ 3. Extract features â”€â”€
        features = self.extractor.extract(detection)

        # â”€â”€ 4. Classify sign â”€â”€
        detected_sign, confidence = "", 0.0
        if mode == "static" and self.static_classifier:
            detected_sign, confidence = self.static_classifier.predict(features.vector)
        elif mode == "dynamic" and self.dynamic_classifier:
            detected_sign, confidence = self.dynamic_classifier.predict(features.vector)

        # â”€â”€ 5. Score against target â”€â”€
        score_result = self.scorer.score(features, target_sign)

        # â”€â”€ 6. Generate feedback â”€â”€
        feedback = self.feedback_gen.generate(score_result)

        # â”€â”€ 7. Build response â”€â”€
        joint_scores = JointScores(**{
            k: v for k, v in score_result.joint_scores.items()
            if k in JointScores.model_fields
        })
        joint_scores.position = score_result.position_score
        joint_scores.orientation = score_result.orientation_score

        landmarks = None
        if include_landmarks and detection.right_hand:
            landmarks = {
                "right_hand": [{"x": lm.x, "y": lm.y, "z": lm.z} for lm in detection.right_hand],
                "left_hand": [{"x": lm.x, "y": lm.y, "z": lm.z} for lm in detection.left_hand] if detection.left_hand else [],
            }

        return AnalyzeFrameResponse(
            hand_detected=True,
            detected_sign=detected_sign,
            confidence=confidence,
            overall_score=score_result.overall_score,
            is_correct=score_result.is_correct,
            joint_scores=joint_scores,
            messages=feedback.messages,
            praise=feedback.praise,
            emoji=feedback.emoji,
            joint_colors=feedback.joint_colors,
            landmarks=landmarks,
        )

    def _decode_frame(self, frame_base64: str) -> np.ndarray:
        """Decode base64 image string to OpenCV BGR numpy array."""
        # Strip data URL prefix if present (e.g. "data:image/jpeg;base64,...")
        if "," in frame_base64:
            frame_base64 = frame_base64.split(",")[1]

        img_bytes = base64.b64decode(frame_base64)
        img_array = np.frombuffer(img_bytes, dtype=np.uint8)
        frame = cv2.imdecode(img_array, cv2.IMREAD_COLOR)

        if frame is None:
            raise ValueError("[ASLPipeline] Failed to decode frame.")
        return frame

    def _no_detection_response(self, target_sign: str) -> AnalyzeFrameResponse:
        return AnalyzeFrameResponse(
            hand_detected=False,
            detected_sign="",
            confidence=0.0,
            overall_score=0.0,
            is_correct=False,
            joint_scores=JointScores(),
            messages=["No hand detected. Make sure your hand is visible in the camera."],
            praise="",
            emoji="ðŸ¤”",
            joint_colors={},
        )

    def release(self):
        self.detector.release()
        print("[ASLPipeline] Released all resources.")
